{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "\n",
    "在这个快速入门中，我们将向您展示如何使用LangChain构建一个简单的LLM应用程序。这个应用程序将会把文本从英语翻译成另一种语言。这是一个相对简单的LLM应用——它只需要一次LLM调用加上一些prompt。尽管如此，这是开始使用LangChain的一个很好的方式——仅通过一些prompt和LLM调用就能构建出许多功能！  \n",
    "\n",
    "这个教程涉及如下内容：  \n",
    "1. 启用LangSmith  \n",
    "2. 使用本地LLM服务  \n",
    "3. 使用模型Streaming模式对话  \n",
    "4. 创建并使用prompt模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这一步需要在LangSmith网站上申请LangSmith api key。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行下面这一步需要在本地搭建LLM服务：  \n",
    "使用ollama下载qwen2或deepseek-r1:32b  \n",
    "后台启动ollama服务： nohup ollama serve  >/dev/null 2>&1 &  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    #model_name=\"deepseek-r1:32b\",\n",
    "    model_name=\"qwen2\",\n",
    "    openai_api_base=\"http://127.0.0.1:11434/v1\",\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 22, 'total_tokens': 26, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-aec81ced-d691-43e4-96ec-210512645d3c-0', usage_metadata={'input_tokens': 22, 'output_tokens': 4, 'total_tokens': 26, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "首先，让我们直接使用该模型。ChatModels是LangChain Runnables的实例，这意味着它们提供了一个与之交互的标准接口。要简单地调用模型，我们可以将消息列表传递给.invoke方法。这种方法允许用户以一种标准化的方式与模型进行互动，使得调用模型变得直接且易于操作。这种方式非常适合用于测试和快速验证想法。\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们启用了LangSmith，我们可以看到这次运行已经被记录到LangSmith中，并且可以查看LangSmith跟踪。LangSmith跟踪报告了令牌使用情况信息、延迟、标准模型参数（如温度）以及其他信息。\n",
    "\n",
    "通过LangSmith，开发者能够详细监控和评估他们的应用程序，不仅有助于优化性能，还能确保模型的安全性和稳定性。这个功能对于持续改进应用程序以及了解模型在实际应用中的表现非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 20, 'total_tokens': 30, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-05a1cd4b-cc8a-4d1b-b63b-ffe7912a6eea-0', usage_metadata={'input_tokens': 20, 'output_tokens': 10, 'total_tokens': 30, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "注意，ChatModels接收消息对象作为输入，并生成消息对象作为输出。除了文本内容外，消息对象还传达了对话角色并保存重要数据，如工具调用和令牌使用计数。\n",
    "LangChain还支持通过字符串或OpenAI格式进行聊天模型输入。以下三种调用方式是等效的：\n",
    "\"\"\"\n",
    "\n",
    "#model.invoke(\"Hello\")\n",
    "\n",
    "#model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C|iao|!||"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "由于chat models是Runnables，它们暴露了一个标准接口，该接口包括异步和流式调用模式。这使我们能够从聊天模型中streaming传输单个token。\n",
    "\"\"\"\n",
    "\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "目前，我们是直接将消息列表传递到语言模型中。这个消息列表是从哪里来的呢？通常，它是由用户输入和应用程序逻辑组合构建的。这种应用程序逻辑通常会获取原始用户输入，并将其转换成准备传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "\n",
    "提示模板是LangChain中设计来辅助进行这种转换的概念。它们接收原始用户输入，并返回准备好传递给语言模型的数据（prompt）。\n",
    "\n",
    "让我们在这里创建一个提示模板。它将接收两个用户变量：\n",
    "- language: 要翻译成的语言\n",
    "- text: 要翻译的文本\n",
    "\n",
    "注意，ChatPromptTemplate支持在单个模板中包含多个消息角色。我们将language参数格式化为系统消息，并将text格式化为用户消息。\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "提示模板的输入是一个字典。下面我们试用这个提示模板，来看看它单独使用时的效果。\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "我们可以看到，它返回了一个由SystemMessage和HumanMessage组成的ChatPromptValue。如果我们想要直接访问这些消息，可以这样做：\n",
    "\"\"\"\n",
    "\n",
    "prompt.to_messages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "最后，我们可以使用这个格式化的prompt调用聊天模型：\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
